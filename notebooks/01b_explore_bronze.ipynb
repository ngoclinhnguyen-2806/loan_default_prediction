{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea253b0-a759-4c5e-bb35-51b81f04af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score, \n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b86b2e9-1014-4822-a4a1-577626e4e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"explore_bronze\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.parquet.mergeSchema\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e9f42e-2708-455b-b7e0-21da89d57f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LOAN_DIR = \"/app/datamart/bronze/lms_loan_daily/\"\n",
    "ATTR_DIR = \"/app/datamart/bronze/features_attributes\"\n",
    "FIN_DIR = \"/app/datamart/bronze/features_financials/\"\n",
    "CLICK_DIR = \"/app/datamart/bronze/feature_clickstream/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a40f70d-aa84-4d09-9145-29556a8aad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate groups: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 7) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----+\n",
      "|Customer_ID|snapshot_date|count|\n",
      "+-----------+-------------+-----+\n",
      "+-----------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Read the bronze clickstream data\n",
    "df_click = spark.read.parquet(CLICK_DIR)\n",
    "\n",
    "# Count duplicates by Customer_ID + snapshot_date\n",
    "dupes = (\n",
    "    df_click.groupBy(\"Customer_ID\", \"snapshot_date\")\n",
    "            .count()\n",
    "            .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(f\"Total duplicate groups: {dupes.count()}\")\n",
    "dupes.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19bc0088-d14f-4186-acd4-8edc9abe1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate groups: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:========>                                                  (1 + 6) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----+\n",
      "|Customer_ID|snapshot_date|count|\n",
      "+-----------+-------------+-----+\n",
      "+-----------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_fin = spark.read.parquet(FIN_DIR)\n",
    "\n",
    "# Count duplicates by Customer_ID + snapshot_date\n",
    "dupes = (\n",
    "    df_fin.groupBy(\"Customer_ID\", \"snapshot_date\")\n",
    "            .count()\n",
    "            .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(f\"Total duplicate groups: {dupes.count()}\")\n",
    "dupes.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d09e9d-b8fa-477f-818d-05e6a25d4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer_ID',\n",
       " 'Annual_Income',\n",
       " 'Monthly_Inhand_Salary',\n",
       " 'Num_Bank_Accounts',\n",
       " 'Num_Credit_Card',\n",
       " 'Interest_Rate',\n",
       " 'Num_of_Loan',\n",
       " 'Type_of_Loan',\n",
       " 'Delay_from_due_date',\n",
       " 'Num_of_Delayed_Payment',\n",
       " 'Changed_Credit_Limit',\n",
       " 'Num_Credit_Inquiries',\n",
       " 'Credit_Mix',\n",
       " 'Outstanding_Debt',\n",
       " 'Credit_Utilization_Ratio',\n",
       " 'Credit_History_Age',\n",
       " 'Payment_of_Min_Amount',\n",
       " 'Total_EMI_per_month',\n",
       " 'Amount_invested_monthly',\n",
       " 'Payment_Behaviour',\n",
       " 'Monthly_Balance',\n",
       " 'snapshot_date',\n",
       " 'year',\n",
       " 'month']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6aee81-a662-4383-8b4f-776d542e6e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate groups: 0\n",
      "+-----------+-------------+-----+\n",
      "|Customer_ID|snapshot_date|count|\n",
      "+-----------+-------------+-----+\n",
      "+-----------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_attr = spark.read.parquet(ATTR_DIR)\n",
    "\n",
    "# Count duplicates by Customer_ID + snapshot_date\n",
    "dupes = (\n",
    "    df_attr.groupBy(\"Customer_ID\", \"snapshot_date\")\n",
    "            .count()\n",
    "            .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(f\"Total duplicate groups: {dupes.count()}\")\n",
    "dupes.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c213e-2b8a-4764-b69c-571fdeed5199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e023e0af-060a-424e-b176-7d4898400bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
