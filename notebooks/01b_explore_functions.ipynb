{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ffbe75-03ea-4072-8525-772f6076cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e6fd0d-01af-4fcd-8fc2-c8862658f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/07 03:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/07 03:45:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"read_parquet\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc51331-83b6-4e1a-b813-219e63be1806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/experiments'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d34859e-fb39-4c33-9ddc-3c713141a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_path = '/app/datamart/silver/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349be77-2227-42a7-b7ef-9207aaefeb8e",
   "metadata": {},
   "source": [
    "# Silver layer - Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4a01e-fa93-4e0c-b367-4bb22dd3fa93",
   "metadata": {},
   "source": [
    "## METHOD 1: Read a specific partition (single date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47357737-767b-480b-a8f2-90595ae0a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_partition(table_name, snapshot_date_str):\n",
    "    \"\"\"\n",
    "    Read a single partition for a specific date\n",
    "    \n",
    "    Args:\n",
    "        table_name: One of ['loan_daily', 'feature_clickstream', \n",
    "                           'features_attributes', 'features_financials']\n",
    "        snapshot_date_str: Date string in format 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    silver_directory = \"/app/datamart/silver/\"\n",
    "    table_path = os.path.join(silver_directory, table_name)\n",
    "    \n",
    "    partition_name = f\"silver_{table_name}_{snapshot_date_str.replace('-','_')}.parquet\"\n",
    "    filepath = os.path.join(table_path, partition_name)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        df = spark.read.parquet(filepath)\n",
    "        print(f\"Loaded {table_name} for {snapshot_date_str}: {df.count()} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d6d53e-41d8-4dda-85ff-d5671d7ef23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded loan_daily for 2023-01-01: 530 rows\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+\n",
      "|             loan_id|Customer_ID|loan_start_date|tenure|installment_num|loan_amt|due_amt|paid_amt|overdue_amt|balance|snapshot_date|mob|installments_missed|first_missed_date|dpd|\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+\n",
      "|CUS_0x1037_2023_0...| CUS_0x1037|     2023-01-01|    10|              0|   10000|    0.0|     0.0|        0.0|10000.0|   2023-01-01|  0|                  0|             NULL|  0|\n",
      "|CUS_0x1069_2023_0...| CUS_0x1069|     2023-01-01|    10|              0|   10000|    0.0|     0.0|        0.0|10000.0|   2023-01-01|  0|                  0|             NULL|  0|\n",
      "|CUS_0x114a_2023_0...| CUS_0x114a|     2023-01-01|    10|              0|   10000|    0.0|     0.0|        0.0|10000.0|   2023-01-01|  0|                  0|             NULL|  0|\n",
      "|CUS_0x1184_2023_0...| CUS_0x1184|     2023-01-01|    10|              0|   10000|    0.0|     0.0|        0.0|10000.0|   2023-01-01|  0|                  0|             NULL|  0|\n",
      "|CUS_0x1297_2023_0...| CUS_0x1297|     2023-01-01|    10|              0|   10000|    0.0|     0.0|        0.0|10000.0|   2023-01-01|  0|                  0|             NULL|  0|\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- loan_start_date: date (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- installment_num: integer (nullable = true)\n",
      " |-- loan_amt: integer (nullable = true)\n",
      " |-- due_amt: double (nullable = true)\n",
      " |-- paid_amt: double (nullable = true)\n",
      " |-- overdue_amt: double (nullable = true)\n",
      " |-- balance: double (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- mob: integer (nullable = true)\n",
      " |-- installments_missed: integer (nullable = true)\n",
      " |-- first_missed_date: date (nullable = true)\n",
      " |-- dpd: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loan = read_single_partition('loan_daily', '2023-01-01')\n",
    "if df_loan:\n",
    "    df_loan.show(5)\n",
    "    df_loan.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0936b44-8f9b-40af-97bc-6d163edb98a0",
   "metadata": {},
   "source": [
    "## METHOD 2: Read all partitions for a table (all dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5986e448-4e28-48b9-b679-1bc1dcb3deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_partitions(table_name):\n",
    "    \"\"\"\n",
    "    Read all partitions for a table across all dates\n",
    "    \n",
    "    Args:\n",
    "        table_name: One of ['loan_daily', 'feature_clickstream', \n",
    "                           'features_attributes', 'features_financials']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all historical data\n",
    "    \"\"\"\n",
    "    silver_directory = \"/app/datamart/silver/\"\n",
    "    table_path = os.path.join(silver_directory, table_name)\n",
    "    \n",
    "    # Get all parquet files\n",
    "    parquet_files = glob.glob(os.path.join(table_path, \"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        # Read all parquet files at once\n",
    "        df = spark.read.parquet(*parquet_files)\n",
    "        print(f\"Loaded all partitions for {table_name}: {df.count()} rows\")\n",
    "        print(f\"Date range: {df.agg({'snapshot_date': 'min'}).collect()[0][0]} to {df.agg({'snapshot_date': 'max'}).collect()[0][0]}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No parquet files found in {table_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5056aab9-3f98-4581-832c-a3a672986024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all partitions for loan_daily: 104288 rows\n",
      "Date range: 2023-01-01 to 2024-12-01\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+----------+\n",
      "|             loan_id|Customer_ID|loan_start_date|tenure|installment_num|loan_amt|due_amt|paid_amt|overdue_amt|balance|snapshot_date|mob|installments_missed|first_missed_date|dpd| asof_date|\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+----------+\n",
      "|CUS_0x100b_2024_0...| CUS_0x100b|     2024-03-01|    10|              6|   10000| 1000.0|  1000.0|        0.0| 4000.0|   2024-09-01|  6|                  0|             NULL|  0|2024-09-01|\n",
      "|CUS_0x1011_2023_1...| CUS_0x1011|     2023-11-01|    10|             10|   10000| 1000.0|  1000.0|        0.0|    0.0|   2024-09-01| 10|                  0|             NULL|  0|2024-09-01|\n",
      "|CUS_0x1013_2023_1...| CUS_0x1013|     2023-12-01|    10|              9|   10000| 1000.0|  1000.0|        0.0| 1000.0|   2024-09-01|  9|                  0|             NULL|  0|2024-09-01|\n",
      "|CUS_0x1018_2023_1...| CUS_0x1018|     2023-11-01|    10|             10|   10000| 1000.0|     0.0|     7000.0| 7000.0|   2024-09-01| 10|                  7|       2024-02-01|213|2024-09-01|\n",
      "|CUS_0x102d_2024_0...| CUS_0x102d|     2024-01-01|    10|              8|   10000| 1000.0|  1000.0|        0.0| 2000.0|   2024-09-01|  8|                  0|             NULL|  0|2024-09-01|\n",
      "+--------------------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+---+-------------------+-----------------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Unique dates: 24\n"
     ]
    }
   ],
   "source": [
    "df_clickstream = read_all_partitions('loan_daily')\n",
    "if df_clickstream:\n",
    "    df_clickstream.show(5)\n",
    "    print(f\"Unique dates: {df_clickstream.select('snapshot_date').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e387dab-c60b-4810-b5e2-c1918afedb80",
   "metadata": {},
   "source": [
    "## METHOD 3: Read with wildcard (Spark handles directory automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509c1bf3-dc3b-4be0-afee-38e44f3af855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_with_wildcard(table_name):\n",
    "    \"\"\"\n",
    "    Let Spark read the entire directory - simplest approach\n",
    "    \n",
    "    Args:\n",
    "        table_name: One of ['loan_daily', 'feature_clickstream', \n",
    "                           'features_attributes', 'features_financials']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all historical data\n",
    "    \"\"\"\n",
    "    silver_directory = \"/app/datamart/silver/\"\n",
    "    table_path = os.path.join(silver_directory, table_name, \"*.parquet\")\n",
    "    \n",
    "    df = spark.read.parquet(table_path)\n",
    "    print(f\"Loaded {table_name}: {df.count()} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b0cc623-455f-41c9-b4f6-5529d27632a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features_attributes: 11974 rows\n",
      "+-----------+------------+-------------+---+\n",
      "|Customer_ID|  Occupation|snapshot_date|Age|\n",
      "+-----------+------------+-------------+---+\n",
      "| CUS_0x10ac|   Developer|   2024-08-01| 29|\n",
      "| CUS_0x10c5|     _______|   2024-08-01| 24|\n",
      "| CUS_0x1145|     Teacher|   2024-08-01| 24|\n",
      "| CUS_0x11ac|  Journalist|   2024-08-01| 26|\n",
      "| CUS_0x122c|Entrepreneur|   2024-08-01| 48|\n",
      "+-----------+------------+-------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_attributes = read_with_wildcard('features_attributes')\n",
    "if df_attributes:\n",
    "    df_attributes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61df40-cd65-491d-a5d2-6f73eda5e2b7",
   "metadata": {},
   "source": [
    "## METHOD 4: Read specific date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4bc5fe8-69e7-4b74-96f2-57871f811866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_date_range(table_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Read partitions within a specific date range\n",
    "    \n",
    "    Args:\n",
    "        table_name: Table name\n",
    "        start_date: Start date string 'YYYY-MM-DD'\n",
    "        end_date: End date string 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Read all partitions\n",
    "    df = read_all_partitions(table_name)\n",
    "    \n",
    "    if df:\n",
    "        # Filter by date range\n",
    "        df_filtered = df.filter(\n",
    "            (col(\"snapshot_date\") >= start_date) & \n",
    "            (col(\"snapshot_date\") <= end_date)\n",
    "        )\n",
    "        print(f\"Filtered to {start_date} - {end_date}: {df_filtered.count()} rows\")\n",
    "        return df_filtered\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98d87e43-1840-4f0d-ae77-1252d2ecafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all partitions for features_financials: 11974 rows\n",
      "Date range: 2023-01-01 to 2024-12-01\n",
      "Filtered to 2023-01-01 - 2023-06-01: 3085 rows\n",
      "+-----------+---------------------+-----------------+---------------+-------------+--------------------+-------------------+--------------------+----------+------------------------+--------------------+---------------------+-------------------+--------------------+------------------+-------------+-------------+-----------+----------------------+--------------------+----------------+-----------------------+-----------------------+------------------------+-----------+-------------------+-------------------------+-----------------------+---------------------+-------------------+----------------------+----------------------------+---------------+----------+\n",
      "|Customer_ID|Monthly_Inhand_Salary|Num_Bank_Accounts|Num_Credit_Card|Interest_Rate|        Type_of_Loan|Delay_from_due_date|Num_Credit_Inquiries|Credit_Mix|Credit_Utilization_Ratio|  Credit_History_Age|Payment_of_Min_Amount|Total_EMI_per_month|   Payment_Behaviour|   Monthly_Balance|snapshot_date|Annual_Income|Num_of_Loan|Num_of_Delayed_Payment|Changed_Credit_Limit|Outstanding_Debt|Amount_invested_monthly|Credit_History_Age_Year|Credit_History_Age_Month|        DTI|loan_type__AutoLoan|loan_type__Credit-Builder|loan_type__PersonalLoan|loan_type__HomeEquity|loan_type__Mortgage|loan_type__StudentLoan|loan_type__DebtConsolidation|loan_type_count| asof_date|\n",
      "+-----------+---------------------+-----------------+---------------+-------------+--------------------+-------------------+--------------------+----------+------------------------+--------------------+---------------------+-------------------+--------------------+------------------+-------------+-------------+-----------+----------------------+--------------------+----------------+-----------------------+-----------------------+------------------------+-----------+-------------------+-------------------------+-----------------------+---------------------+-------------------+----------------------+----------------------------+---------------+----------+\n",
      "| CUS_0x1037|           1086.42375|                5|              4|            2|Credit-Builder Lo...|                 13|                 3.0|      Good|       40.69769934536393|19 Years and 9 Mo...|                   No| 33.797020629881075|Low_spent_Small_v...| 284.3801148556844|   2023-01-01|    15989.085|          4|                    15|                 0.5|          665.82|               80.46524|                  19.75|                     237| 0.04164216|                  1|                        0|                      0|                    0|                  0|                     0|                           0|              1|2023-01-01|\n",
      "| CUS_0x1069|             4799.445|                4|              6|           10|Personal Loan, Au...|                  9|                 5.0|  Standard|       25.23314363376225|30 Years and 8 Mo...|                  Yes|  139.8850133927109|High_spent_Small_...| 434.8488737169301|   2023-01-01|     58637.34|        119|                    17|               12.56|           208.8|              165.21062|     30.666666666666668|                     368|0.003560871|                  1|                        0|                      1|                    0|                  0|                     0|                           0|              2|2023-01-01|\n",
      "| CUS_0x114a|             1230.455|                0|              7|            2|Student Loan, and...|                 14|                 0.0|      Good|       27.52511297681479|15 Years and 9 Mo...|                   No| 20.301653868439384|Low_spent_Small_v...|327.96536605522886|   2023-01-01|     15305.46|          2|                     2|               15.95|          642.42|               64.77848|                  15.75|                     189|0.041973256|                  0|                        0|                      0|                    0|                  0|                     1|                           0|              1|2023-01-01|\n",
      "| CUS_0x1184|   1396.6229166666665|                3|              5|           11|Student Loan, Mor...|                 10|                 4.0|      Good|       26.68978981173729|32 Years and 8 Mo...|                   No|  42.60688196261637|             Unknown| 313.5944656613208|   2023-01-01|    19867.475|          3|                     9|                6.74|          707.29|              23.460943|     32.666666666666664|                     392|0.035600398|                  0|                        0|                      0|                    0|                  0|                     1|                           0|              1|2023-01-01|\n",
      "| CUS_0x1297|             4881.505|                9|              8|           30|Payday Loan, Pers...|                 61|                11.0|       Bad|       25.74214265617326|13 Years and 8 Mo...|                  Yes|  296.2841357039575|High_spent_Medium...|388.04518664772837|   2023-01-01|     57738.06|          9|                    24|               14.27|         3916.47|               53.82118|     13.666666666666666|                     164| 0.06783169|                  0|                        0|                      1|                    0|                  0|                     1|                           0|              2|2023-01-01|\n",
      "+-----------+---------------------+-----------------+---------------+-------------+--------------------+-------------------+--------------------+----------+------------------------+--------------------+---------------------+-------------------+--------------------+------------------+-------------+-------------+-----------+----------------------+--------------------+----------------+-----------------------+-----------------------+------------------------+-----------+-------------------+-------------------------+-----------------------+---------------------+-------------------+----------------------+----------------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_financials = read_date_range('features_financials', '2023-01-01', '2023-06-01')\n",
    "if df_financials:\n",
    "    df_financials.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa4a759a-ccb2-4939-a2e3-4a590dc4c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Monthly_Inhand_Salary: double (nullable = true)\n",
      " |-- Num_Bank_Accounts: integer (nullable = true)\n",
      " |-- Num_Credit_Card: integer (nullable = true)\n",
      " |-- Interest_Rate: integer (nullable = true)\n",
      " |-- Type_of_Loan: string (nullable = true)\n",
      " |-- Delay_from_due_date: integer (nullable = true)\n",
      " |-- Num_Credit_Inquiries: double (nullable = true)\n",
      " |-- Credit_Mix: string (nullable = true)\n",
      " |-- Credit_Utilization_Ratio: double (nullable = true)\n",
      " |-- Credit_History_Age: string (nullable = true)\n",
      " |-- Payment_of_Min_Amount: string (nullable = true)\n",
      " |-- Total_EMI_per_month: double (nullable = true)\n",
      " |-- Payment_Behaviour: string (nullable = true)\n",
      " |-- Monthly_Balance: double (nullable = true)\n",
      " |-- snapshot_date: date (nullable = true)\n",
      " |-- Annual_Income: float (nullable = true)\n",
      " |-- Num_of_Loan: integer (nullable = true)\n",
      " |-- Num_of_Delayed_Payment: integer (nullable = true)\n",
      " |-- Changed_Credit_Limit: float (nullable = true)\n",
      " |-- Outstanding_Debt: float (nullable = true)\n",
      " |-- Amount_invested_monthly: float (nullable = true)\n",
      " |-- Credit_History_Age_Year: double (nullable = true)\n",
      " |-- Credit_History_Age_Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_financials.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe54d9-d285-4433-ae5d-d6aa1ac7ae87",
   "metadata": {},
   "source": [
    "## METHOD 5: Read and convert to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c76136-244b-4b94-b95c-868aeaf921b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_pandas(table_name, snapshot_date_str=None):\n",
    "    \"\"\"\n",
    "    Read parquet and convert to Pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        table_name: Table name\n",
    "        snapshot_date_str: Optional specific date. If None, reads all dates\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if snapshot_date_str:\n",
    "        df_spark = read_single_partition(table_name, snapshot_date_str)\n",
    "    else:\n",
    "        df_spark = read_all_partitions(table_name)\n",
    "    \n",
    "    if df_spark:\n",
    "        # Convert to Pandas\n",
    "        df_pandas = df_spark.toPandas()\n",
    "        print(f\"Converted to Pandas: {len(df_pandas)} rows\")\n",
    "        return df_pandas\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec070f9-70c9-4718-b99f-32b28c8bc7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded loan_daily for 2023-01-01: 530 rows\n",
      "Converted to Pandas: 530 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>loan_start_date</th>\n",
       "      <th>tenure</th>\n",
       "      <th>installment_num</th>\n",
       "      <th>loan_amt</th>\n",
       "      <th>due_amt</th>\n",
       "      <th>paid_amt</th>\n",
       "      <th>overdue_amt</th>\n",
       "      <th>balance</th>\n",
       "      <th>snapshot_date</th>\n",
       "      <th>mob</th>\n",
       "      <th>installments_missed</th>\n",
       "      <th>first_missed_date</th>\n",
       "      <th>dpd</th>\n",
       "      <th>asof_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUS_0x1037_2023_01_01</td>\n",
       "      <td>CUS_0x1037</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUS_0x1069_2023_01_01</td>\n",
       "      <td>CUS_0x1069</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUS_0x114a_2023_01_01</td>\n",
       "      <td>CUS_0x114a</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUS_0x1184_2023_01_01</td>\n",
       "      <td>CUS_0x1184</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUS_0x1297_2023_01_01</td>\n",
       "      <td>CUS_0x1297</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loan_id Customer_ID loan_start_date  tenure  installment_num  \\\n",
       "0  CUS_0x1037_2023_01_01  CUS_0x1037      2023-01-01      10                0   \n",
       "1  CUS_0x1069_2023_01_01  CUS_0x1069      2023-01-01      10                0   \n",
       "2  CUS_0x114a_2023_01_01  CUS_0x114a      2023-01-01      10                0   \n",
       "3  CUS_0x1184_2023_01_01  CUS_0x1184      2023-01-01      10                0   \n",
       "4  CUS_0x1297_2023_01_01  CUS_0x1297      2023-01-01      10                0   \n",
       "\n",
       "   loan_amt  due_amt  paid_amt  overdue_amt  balance snapshot_date  mob  \\\n",
       "0     10000      0.0       0.0          0.0  10000.0    2023-01-01    0   \n",
       "1     10000      0.0       0.0          0.0  10000.0    2023-01-01    0   \n",
       "2     10000      0.0       0.0          0.0  10000.0    2023-01-01    0   \n",
       "3     10000      0.0       0.0          0.0  10000.0    2023-01-01    0   \n",
       "4     10000      0.0       0.0          0.0  10000.0    2023-01-01    0   \n",
       "\n",
       "   installments_missed first_missed_date  dpd   asof_date  \n",
       "0                    0              None    0  2023-01-01  \n",
       "1                    0              None    0  2023-01-01  \n",
       "2                    0              None    0  2023-01-01  \n",
       "3                    0              None    0  2023-01-01  \n",
       "4                    0              None    0  2023-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 530 entries, 0 to 529\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   loan_id              530 non-null    object \n",
      " 1   Customer_ID          530 non-null    object \n",
      " 2   loan_start_date      530 non-null    object \n",
      " 3   tenure               530 non-null    int32  \n",
      " 4   installment_num      530 non-null    int32  \n",
      " 5   loan_amt             530 non-null    int32  \n",
      " 6   due_amt              530 non-null    float64\n",
      " 7   paid_amt             530 non-null    float64\n",
      " 8   overdue_amt          530 non-null    float64\n",
      " 9   balance              530 non-null    float64\n",
      " 10  snapshot_date        530 non-null    object \n",
      " 11  mob                  530 non-null    int32  \n",
      " 12  installments_missed  530 non-null    int32  \n",
      " 13  first_missed_date    0 non-null      object \n",
      " 14  dpd                  530 non-null    int32  \n",
      " 15  asof_date            530 non-null    object \n",
      "dtypes: float64(4), int32(6), object(6)\n",
      "memory usage: 54.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pandas = read_to_pandas('loan_daily', '2023-01-01')\n",
    "if df_pandas is not None:\n",
    "    display(df_pandas.head())\n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    df_pandas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df0c60-eec5-4983-81ad-b5e56e40d497",
   "metadata": {},
   "source": [
    "# Bronze Layer - CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac96e7d2-58af-4031-8da1-6239a5927c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, lit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d680fb-4ed7-4e0e-8258-9cbee4f747eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"read_bronze\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a7fbeb-54f4-4ecd-8443-a7acd0ba5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BRONZE_DIR = \"/app/datamart/bronze/\"\n",
    "TABLE_NAME = \"lms_loan_daily\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c511b-9f77-43b0-ab2e-54ae48aac864",
   "metadata": {},
   "source": [
    "## METHOD 1: Read Single Partition (One Snapshot Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0000209f-981f-4789-abe9-3c4d50f0778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_partition_csv(bronze_dir, table_name, snapshot_date_str, spark):\n",
    "    \"\"\"\n",
    "    Read a single CSV partition for a specific snapshot date\n",
    "    \n",
    "    Args:\n",
    "        bronze_dir: Bronze directory path\n",
    "        table_name: Table name (e.g., 'lms_loan_daily')\n",
    "        snapshot_date_str: Date string 'YYYY-MM-DD'\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame for that specific date\n",
    "    \"\"\"\n",
    "    # Construct the filename\n",
    "    partition_name = f\"bronze_{table_name}_{snapshot_date_str.replace('-', '_')}.csv\"\n",
    "    filepath = os.path.join(bronze_dir, table_name, partition_name)\n",
    "    \n",
    "    print(f\"\\nReading file: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"❌ File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the CSV\n",
    "    df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "    \n",
    "    print(f\"✓ Loaded {df.count():,} rows from {snapshot_date_str}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54bc4e8c-8e89-41bf-ab49-43cded4d247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file: /app/datamart/bronze/lms_loan_daily/bronze_lms_loan_daily_2023_01_01.csv\n",
      "✓ Loaded 0 rows from 2023-01-01\n",
      "\n",
      "Sample data:\n",
      "+-------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+\n",
      "|loan_id|Customer_ID|loan_start_date|tenure|installment_num|loan_amt|due_amt|paid_amt|overdue_amt|balance|snapshot_date|\n",
      "+-------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+\n",
      "+-------+-----------+---------------+------+---------------+--------+-------+--------+-----------+-------+-------------+\n",
      "\n",
      "\n",
      "Columns: ['loan_id', 'Customer_ID', 'loan_start_date', 'tenure', 'installment_num', 'loan_amt', 'due_amt', 'paid_amt', 'overdue_amt', 'balance', 'snapshot_date']\n"
     ]
    }
   ],
   "source": [
    "snapshot_date = \"2023-01-01\"\n",
    "df_single = read_single_partition_csv(BRONZE_DIR, TABLE_NAME, snapshot_date, spark)\n",
    "\n",
    "if df_single:\n",
    "    print(\"\\nSample data:\")\n",
    "    df_single.show(5)\n",
    "    print(f\"\\nColumns: {df_single.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b89199-a737-49e2-95fe-f5f3048ac43c",
   "metadata": {},
   "source": [
    "## METHOD 2: Read MULTIPLE partitions (date range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed4dc5b0-20c4-4e05-aca0-c25f045690a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiple_partitions_csv(bronze_dir, table_name, start_date, end_date, spark):\n",
    "    \"\"\"\n",
    "    Read multiple CSV partitions for a date range\n",
    "    \n",
    "    Args:\n",
    "        bronze_dir: Bronze directory path\n",
    "        table_name: Table name\n",
    "        start_date: Start date string 'YYYY-MM-DD'\n",
    "        end_date: End date string 'YYYY-MM-DD'\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all dates\n",
    "    \"\"\"\n",
    "    table_path = os.path.join(bronze_dir, table_name)\n",
    "    \n",
    "    # Get all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(table_path, \"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} total partitions\")\n",
    "    \n",
    "    # Filter files by date range\n",
    "    selected_files = []\n",
    "    for filepath in all_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        # Extract date from filename: bronze_lms_loan_daily_2023_01_01.csv\n",
    "        date_part = filename.replace(\"bronze_\", \"\").replace(f\"{table_name}_\", \"\").replace(\".csv\", \"\")\n",
    "        file_date = date_part.replace(\"_\", \"-\")  # Convert 2023_01_01 to 2023-01-01\n",
    "        \n",
    "        if start_date <= file_date <= end_date:\n",
    "            selected_files.append(filepath)\n",
    "    \n",
    "    print(f\"Selected {len(selected_files)} partitions in range {start_date} to {end_date}\")\n",
    "    \n",
    "    if not selected_files:\n",
    "        print(\"❌ No files found in date range\")\n",
    "        return None\n",
    "    \n",
    "    # Read all selected files\n",
    "    dfs = []\n",
    "    for filepath in sorted(selected_files):\n",
    "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "        \n",
    "        # Extract date and add as column\n",
    "        filename = os.path.basename(filepath)\n",
    "        date_part = filename.replace(\"bronze_\", \"\").replace(f\"{table_name}_\", \"\").replace(\".csv\", \"\")\n",
    "        file_date = date_part.replace(\"_\", \"-\")\n",
    "        \n",
    "        df = df.withColumn(\"snapshot_date\", lit(file_date))\n",
    "        dfs.append(df)\n",
    "        \n",
    "        print(f\"  ✓ Loaded {filepath}: {df.count():,} rows\")\n",
    "    \n",
    "    # Union all dataframes\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    \n",
    "    print(f\"\\n✓ Combined total: {combined_df.count():,} rows\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d76f89-3083-413c-af60-fda2f8239507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple = read_multiple_partitions_csv(\n",
    "    BRONZE_DIR, TABLE_NAME, \n",
    "    \"2023-01-01\", \"2023-03-01\", \n",
    "    spark\n",
    ")\n",
    "\n",
    "if df_multiple:\n",
    "    print(\"\\nDate distribution:\")\n",
    "    df_multiple.groupBy(\"snapshot_date\").count().orderBy(\"snapshot_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0192a0-f179-4d5b-a90c-485bfb04e6bc",
   "metadata": {},
   "source": [
    "## METHOD 3: Read ALL partitions (entire history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120ab8ce-4ff2-4412-bf31-e3c3a0066799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_partitions_csv(bronze_dir, table_name, spark):\n",
    "    \"\"\"\n",
    "    Read all CSV partitions (entire history)\n",
    "    \n",
    "    Args:\n",
    "        bronze_dir: Bronze directory path\n",
    "        table_name: Table name\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all historical data\n",
    "    \"\"\"\n",
    "    table_path = os.path.join(bronze_dir, table_name)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    all_files = glob.glob(os.path.join(table_path, \"*.csv\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} partitions\")\n",
    "    \n",
    "    if not all_files:\n",
    "        print(\"❌ No CSV files found\")\n",
    "        return None\n",
    "    \n",
    "    # Read all files\n",
    "    dfs = []\n",
    "    for filepath in sorted(all_files):\n",
    "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "        \n",
    "        # Extract and add snapshot_date\n",
    "        filename = os.path.basename(filepath)\n",
    "        date_part = filename.replace(\"bronze_\", \"\").replace(f\"{table_name}_\", \"\").replace(\".csv\", \"\")\n",
    "        file_date = date_part.replace(\"_\", \"-\")\n",
    "        \n",
    "        df = df.withColumn(\"snapshot_date\", lit(file_date))\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Union all\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    \n",
    "    total_rows = combined_df.count()\n",
    "    unique_dates = combined_df.select(\"snapshot_date\").distinct().count()\n",
    "    \n",
    "    print(f\"✓ Loaded {total_rows:,} total rows\")\n",
    "    print(f\"✓ Spanning {unique_dates} unique dates\")\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e996e8a-167d-4b86-92fa-3180479d3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 24 partitions\n",
      "✓ Loaded 104,288 total rows\n",
      "✓ Spanning 24 unique dates\n",
      "\n",
      "Date range:\n",
      "+-------+-------------+\n",
      "|summary|snapshot_date|\n",
      "+-------+-------------+\n",
      "|    min|   2023-01-01|\n",
      "|    max|   2024-12-01|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all = read_all_partitions_csv(BRONZE_DIR, TABLE_NAME, spark)\n",
    "\n",
    "if df_all:\n",
    "    print(\"\\nDate range:\")\n",
    "    df_all.select(\"snapshot_date\").summary(\"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca73d9f-4ee6-4231-b905-9d3b66a0b2de",
   "metadata": {},
   "source": [
    "## METHOD 4: Using Pandas (for smaller datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d968f-1823-4e07-9669-d219da903173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partition_pandas(bronze_dir, table_name, snapshot_date_str):\n",
    "    \"\"\"\n",
    "    Read a single partition using Pandas\n",
    "    Faster for small to medium datasets\n",
    "    \n",
    "    Args:\n",
    "        bronze_dir: Bronze directory path\n",
    "        table_name: Table name\n",
    "        snapshot_date_str: Date string 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    partition_name = f\"bronze_{table_name}_{snapshot_date_str.replace('-', '_')}.csv\"\n",
    "    filepath = os.path.join(bronze_dir, table_name, partition_name)\n",
    "    \n",
    "    print(f\"\\nReading with Pandas: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"❌ File not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df['snapshot_date'] = snapshot_date_str\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df):,} rows\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434adc2b-3472-44ae-8c9d-24a4059afc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = read_partition_pandas(BRONZE_DIR, TABLE_NAME, \"2023-01-01\")\n",
    "\n",
    "if df_pandas is not None:\n",
    "    print(\"\\nPandas DataFrame info:\")\n",
    "    print(df_pandas.head())\n",
    "    print(f\"\\nShape: {df_pandas.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077bcdf-46d4-4b4c-a4f9-f41aa1edb22a",
   "metadata": {},
   "source": [
    "## METHOD 5: Read specific partitions by list of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ba5a4-2b1d-4a7e-83a7-b10e32fe2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_specific_dates_csv(bronze_dir, table_name, date_list, spark):\n",
    "    \"\"\"\n",
    "    Read specific partitions by providing a list of dates\n",
    "    \n",
    "    Args:\n",
    "        bronze_dir: Bronze directory path\n",
    "        table_name: Table name\n",
    "        date_list: List of date strings ['YYYY-MM-DD', ...]\n",
    "        spark: SparkSession\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading {len(date_list)} specific partitions:\")\n",
    "    \n",
    "    dfs = []\n",
    "    for date_str in date_list:\n",
    "        partition_name = f\"bronze_{table_name}_{date_str.replace('-', '_')}.csv\"\n",
    "        filepath = os.path.join(bronze_dir, table_name, partition_name)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"  ⚠ Skipping {date_str}: file not found\")\n",
    "            continue\n",
    "        \n",
    "        df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "        df = df.withColumn(\"snapshot_date\", lit(date_str))\n",
    "        dfs.append(df)\n",
    "        \n",
    "        print(f\"  ✓ {date_str}: {df.count():,} rows\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"❌ No valid partitions found\")\n",
    "        return None\n",
    "    \n",
    "    # Union all\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    \n",
    "    print(f\"\\n✓ Total: {combined_df.count():,} rows\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e883e-9b0e-4842-8f55-af29d060ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_dates = [\"2023-01-01\", \"2023-03-01\", \"2023-06-01\"]\n",
    "df_specific = read_specific_dates_csv(BRONZE_DIR, TABLE_NAME, specific_dates, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6069c7-1237-4304-8a5f-506dc8d0a35b",
   "metadata": {},
   "source": [
    "# Test Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c37115dc-5852-4fb5-bb50-65355096a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:14:18,120 - __main__ - INFO - Kept 'Type_of_Loan' as string (only 0.00% numeric after cleaning)\n",
      "2025-10-07 05:14:18,563 - __main__ - INFO - Kept 'Credit_Mix' as string (only 0.00% numeric after cleaning)\n",
      "2025-10-07 05:14:19,464 - __main__ - INFO - Auto-cast 'Credit_History_Age' -> Integer (cleaned non-numeric chars)\n",
      "2025-10-07 05:14:19,924 - __main__ - INFO - Kept 'Payment_of_Min_Amount' as string (only 0.00% numeric after cleaning)\n",
      "2025-10-07 05:14:20,333 - __main__ - INFO - Kept 'Payment_Behaviour' as string (only 0.00% numeric after cleaning)\n"
     ]
    }
   ],
   "source": [
    "# --- Standard libraries ---\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# --- PySpark core & types ---\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, when\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, DateType, DecimalType\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING\n",
    "# =============================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def upcast_floats_to_double(df):\n",
    "    for f in df.schema.fields:\n",
    "        if isinstance(f.dataType, (FloatType, DecimalType)):\n",
    "            df = df.withColumn(f.name, col(f.name).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "def cast_to_numeric(df, exclude=(\"Customer_ID\", \"snapshot_date\"), numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Auto-detect numeric-looking string columns, clean, and cast to Integer/Double.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "        candidates = [c for c in string_cols if c not in exclude]\n",
    "\n",
    "        for c in candidates:\n",
    "            cleaned = regexp_replace(col(c), r\"[^0-9\\.\\-]+\", \"\")\n",
    "            cleaned = trim(cleaned)\n",
    "\n",
    "            cast_col = when(F.length(cleaned) == 0, None).otherwise(cleaned).cast(DoubleType())\n",
    "            tmp = f\"__num_{c}\"\n",
    "\n",
    "            ratio = df.withColumn(tmp, cast_col) \\\n",
    "                      .select((F.count(tmp) / F.count(F.lit(1))).alias(\"ratio\")) \\\n",
    "                      .collect()[0][\"ratio\"]\n",
    "\n",
    "            if ratio is not None and ratio >= numeric_threshold:\n",
    "                df_with = df.withColumn(tmp, cast_col)\n",
    "                max_frac = df_with.select(\n",
    "                    F.max(F.abs(col(tmp) - F.floor(col(tmp)))).alias(\"max_frac\")\n",
    "                ).collect()[0][\"max_frac\"]\n",
    "\n",
    "                is_integer = (max_frac is None) or (float(max_frac) == 0.0)\n",
    "                target_type = IntegerType() if is_integer else DoubleType()\n",
    "\n",
    "                df = df_with.drop(c).withColumn(c, col(tmp).cast(target_type)).drop(tmp)\n",
    "                logger.info(\n",
    "                    \"Auto-cast '%s' -> %s (cleaned non-numeric chars)\",\n",
    "                    c, \"Integer\" if is_integer else \"Double\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Kept '%s' as string (only %.2f%% numeric after cleaning)\",\n",
    "                    c, (ratio or 0.0) * 100\n",
    "                )\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in transformation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "df_financias = cast_to_numeric(df_financials, exclude=(\"Customer_ID\", \"snapshot_date\"), numeric_threshold=0.9)\n",
    "df_financials = upcast_floats_to_double(df_financials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f25406-0e7a-4e93-950c-e3cf9151ff95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Customer_ID: string, Monthly_Inhand_Salary: double, Num_Bank_Accounts: int, Num_Credit_Card: int, Interest_Rate: int, Type_of_Loan: string, Delay_from_due_date: int, Num_Credit_Inquiries: double, Credit_Mix: string, Credit_Utilization_Ratio: double, Credit_History_Age: string, Payment_of_Min_Amount: string, Total_EMI_per_month: double, Payment_Behaviour: string, Monthly_Balance: double, snapshot_date: date, Annual_Income: double, Num_of_Loan: int, Num_of_Delayed_Payment: int, Changed_Credit_Limit: double, Outstanding_Debt: double, Amount_invested_monthly: double, Credit_History_Age_Year: double, Credit_History_Age_Month: int, DTI: double, loan_type__AutoLoan: int, loan_type__Credit-Builder: int, loan_type__PersonalLoan: int, loan_type__HomeEquity: int, loan_type__Mortgage: int, loan_type__StudentLoan: int, loan_type__DebtConsolidation: int, loan_type_count: int, asof_date: date]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_financials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40a524ca-1aa0-4983-9b8f-c1be6db49aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Spark session stopped\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"\\n✓ Spark session stopped\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
