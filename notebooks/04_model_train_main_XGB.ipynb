{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32fb3bc6-4166-4893-88e1-0d3140df5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206071-5f00-4c3b-be13-55c54db8e336",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f3a464-fe45-477b-8815-cebe102228f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_train_date': datetime.datetime(2024, 8, 1, 0, 0),\n",
      " 'model_train_date_str': '2024-08-01',\n",
      " 'oot_end_date': datetime.datetime(2024, 7, 31, 0, 0),\n",
      " 'oot_period_months': 2,\n",
      " 'oot_start_date': datetime.datetime(2024, 6, 1, 0, 0),\n",
      " 'train_test_end_date': datetime.datetime(2024, 5, 31, 0, 0),\n",
      " 'train_test_period_months': 12,\n",
      " 'train_test_ratio': 0.8,\n",
      " 'train_test_start_date': datetime.datetime(2023, 6, 1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "# set up config\n",
    "model_train_date_str = \"2024-08-01\"\n",
    "train_test_period_months = 12\n",
    "oot_period_months = 2\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "config = {}\n",
    "config[\"model_train_date_str\"] = model_train_date_str\n",
    "config[\"train_test_period_months\"] = train_test_period_months\n",
    "config[\"oot_period_months\"] =  oot_period_months\n",
    "config[\"model_train_date\"] =  datetime.strptime(model_train_date_str, \"%Y-%m-%d\")\n",
    "config[\"oot_end_date\"] =  config['model_train_date'] - timedelta(days = 1)\n",
    "config[\"oot_start_date\"] =  config['model_train_date'] - relativedelta(months = oot_period_months)\n",
    "config[\"train_test_end_date\"] =  config[\"oot_start_date\"] - timedelta(days = 1)\n",
    "config[\"train_test_start_date\"] =  config[\"oot_start_date\"] - relativedelta(months = train_test_period_months)\n",
    "config[\"train_test_ratio\"] = train_test_ratio \n",
    "\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {},
   "source": [
    "## get label store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0abc72e2-90ed-46da-8c6e-599573d54049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_count: 8974\n",
      "+--------------------+-----------+-----+----------+-------------+\n",
      "|             loan_id|Customer_ID|label| label_def|snapshot_date|\n",
      "+--------------------+-----------+-----+----------+-------------+\n",
      "|CUS_0x1037_2023_0...| CUS_0x1037|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1069_2023_0...| CUS_0x1069|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x114a_2023_0...| CUS_0x114a|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1184_2023_0...| CUS_0x1184|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1297_2023_0...| CUS_0x1297|    1|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x12fb_2023_0...| CUS_0x12fb|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1325_2023_0...| CUS_0x1325|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1341_2023_0...| CUS_0x1341|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1375_2023_0...| CUS_0x1375|    1|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x13a8_2023_0...| CUS_0x13a8|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x13ef_2023_0...| CUS_0x13ef|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1440_2023_0...| CUS_0x1440|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1443_2023_0...| CUS_0x1443|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x145a_2023_0...| CUS_0x145a|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1492_2023_0...| CUS_0x1492|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x153d_2023_0...| CUS_0x153d|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1567_2023_0...| CUS_0x1567|    0|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x15ad_2023_0...| CUS_0x15ad|    1|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x1630_2023_0...| CUS_0x1630|    1|30dpd_6mob|   2023-07-01|\n",
      "|CUS_0x169c_2023_0...| CUS_0x169c|    0|30dpd_6mob|   2023-07-01|\n",
      "+--------------------+-----------+-----+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# connect to label store\n",
    "folder_path = \"/app/datamart/gold/label_store/\"\n",
    "files_list = [folder_path+os.path.basename(f) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "label_store_sdf = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "# label_store_sdf = spark.read.parquet(folder_path)\n",
    "print(\"row_count:\",label_store_sdf.count())\n",
    "\n",
    "label_store_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted labels_sdf 6443 2023-06-01 00:00:00 2024-07-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# extract label store\n",
    "labels_sdf = label_store_sdf.filter((col(\"snapshot_date\") >= config[\"train_test_start_date\"]) & (col(\"snapshot_date\") <= config[\"oot_end_date\"]))\n",
    "\n",
    "print(\"extracted labels_sdf\", labels_sdf.count(), config[\"train_test_start_date\"], config[\"oot_end_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfee45c-3612-4c9b-9f8e-fbbf3caa3bc5",
   "metadata": {},
   "source": [
    "## get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d149c8-d7b6-4fe5-a356-54f2df3fd61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m folder_path = \u001b[33m\"\u001b[39m\u001b[33m/app/datamart/gold/feature_store/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m files_list = [folder_path+os.path.basename(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m glob.glob(os.path.join(folder_path, \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m))]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m feature_store_sdf = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfiles_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrow_count:\u001b[39m\u001b[33m\"\u001b[39m,feature_store_sdf.count())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "# connect to label store\n",
    "folder_path = \"/app/datamart/gold/feature_store/\"\n",
    "files_list = [folder_path+os.path.basename(f) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "feature_store_sdf = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "print(\"row_count:\",feature_store_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e4af8-88ea-4c8a-ba37-2a47c020483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d8e7e-f374-42b6-8617-64fffc98b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label store\n",
    "features_sdf = feature_store_sdf.filter((col(\"application_date\") >= config[\"train_test_start_date\"]) & (col(\"application_date\") <= config[\"oot_end_date\"]))\n",
    "\n",
    "print(\"extracted features_sdf\", features_sdf.count(), config[\"train_test_start_date\"], config[\"oot_end_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6f2147-67d1-4a7a-9ec1-f2696c726bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6ada0-efa8-406a-895c-30a86c2d9565",
   "metadata": {},
   "source": [
    "## prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65d173-37c1-4c37-83de-d77de4f60325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for modeling\n",
    "data_pdf = labels_sdf.join(features_sdf, on=[\"Customer_ID\"], how=\"left\").toPandas()\n",
    "data_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076f5b8-b069-4c90-8300-915ed99671a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train - test - oot\n",
    "oot_pdf = data_pdf[(data_pdf['snapshot_date'] >= config[\"oot_start_date\"].date()) & (data_pdf['snapshot_date'] <= config[\"oot_end_date\"].date())]\n",
    "train_test_pdf = data_pdf[(data_pdf['snapshot_date'] >= config[\"train_test_start_date\"].date()) & (data_pdf['snapshot_date'] <= config[\"train_test_end_date\"].date())]\n",
    "\n",
    "feature_cols = [fe_col for fe_col in data_pdf.columns if fe_col.startswith('fe_')]\n",
    "\n",
    "X_oot = oot_pdf[feature_cols]\n",
    "y_oot = oot_pdf[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_test_pdf[feature_cols], train_test_pdf[\"label\"], \n",
    "    test_size= 1 - config[\"train_test_ratio\"],\n",
    "    random_state=88,     # Ensures reproducibility\n",
    "    shuffle=True,        # Shuffle the data before splitting\n",
    "    stratify=train_test_pdf[\"label\"]           # Stratify based on the label column\n",
    ")\n",
    "\n",
    "\n",
    "print('X_train', X_train.shape[0])\n",
    "print('X_test', X_test.shape[0])\n",
    "print('X_oot', X_oot.shape[0])\n",
    "print('y_train', y_train.shape[0], round(y_train.mean(),2))\n",
    "print('y_test', y_test.shape[0], round(y_test.mean(),2))\n",
    "print('y_oot', y_oot.shape[0], round(y_oot.mean(),2))\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14784ef3-512e-475b-8c56-e1238d305814",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572337a5-8d35-4409-80c0-22fda27810b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up standard scalar preprocessing\n",
    "scaler = StandardScaler()\n",
    "\n",
    "transformer_stdscaler = scaler.fit(X_train) # Q which should we use? train? test? oot? all?\n",
    "\n",
    "# transform data\n",
    "X_train_processed = transformer_stdscaler.transform(X_train)\n",
    "X_test_processed = transformer_stdscaler.transform(X_test)\n",
    "X_oot_processed = transformer_stdscaler.transform(X_oot)\n",
    "\n",
    "print('X_train_processed', X_train_processed.shape[0])\n",
    "print('X_test_processed', X_test_processed.shape[0])\n",
    "print('X_oot_processed', X_oot_processed.shape[0])\n",
    "\n",
    "pd.DataFrame(X_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90383aa5-78a6-46dd-aade-a1244adeafd0",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ac962-0d8c-4d8c-8f06-ab4b43cf0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(eval_metric='logloss', random_state=88)\n",
    "\n",
    "# Define the hyperparameter space to search\n",
    "param_dist = {\n",
    "    'n_estimators': [25, 50],\n",
    "    'max_depth': [2, 3],  # lower max_depth to simplify the model\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "    'gamma': [0, 0.1],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Create a scorer based on AUC score\n",
    "auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Set up the random search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=auc_scorer,\n",
    "    n_iter=10,  # Number of iterations for random search\n",
    "    cv=3,       # Number of folds in cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Use all available cores\n",
    ")\n",
    "\n",
    "# Perform the random search\n",
    "random_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Output the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best AUC score: \", random_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the train set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_train_processed)[:, 1]\n",
    "train_auc_score = roc_auc_score(y_train, y_pred_proba)\n",
    "print(\"Train AUC score: \", train_auc_score)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_test_processed)[:, 1]\n",
    "test_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"Test AUC score: \", test_auc_score)\n",
    "\n",
    "# Evaluate the model on the oot set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_oot_processed)[:, 1]\n",
    "oot_auc_score = roc_auc_score(y_oot, y_pred_proba)\n",
    "print(\"OOT AUC score: \", oot_auc_score)\n",
    "\n",
    "print(\"TRAIN GINI score: \", round(2*train_auc_score-1,3))\n",
    "print(\"Test GINI score: \", round(2*test_auc_score-1,3))\n",
    "print(\"OOT GINI score: \", round(2*oot_auc_score-1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846548ca-a1bb-4625-854e-ac39068f3821",
   "metadata": {},
   "source": [
    "## prepare model artefact to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a720c7-df26-41be-935e-06904983c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artefact = {}\n",
    "\n",
    "model_artefact['model'] = best_model\n",
    "model_artefact['model_version'] = \"credit_model_\"+config[\"model_train_date_str\"].replace('-','_')\n",
    "model_artefact['preprocessing_transformers'] = {}\n",
    "model_artefact['preprocessing_transformers']['stdscaler'] = transformer_stdscaler\n",
    "model_artefact['data_dates'] = config\n",
    "model_artefact['data_stats'] = {}\n",
    "model_artefact['data_stats']['X_train'] = X_train.shape[0]\n",
    "model_artefact['data_stats']['X_test'] = X_test.shape[0]\n",
    "model_artefact['data_stats']['X_oot'] = X_oot.shape[0]\n",
    "model_artefact['data_stats']['y_train'] = round(y_train.mean(),2)\n",
    "model_artefact['data_stats']['y_test'] = round(y_test.mean(),2)\n",
    "model_artefact['data_stats']['y_oot'] = round(y_oot.mean(),2)\n",
    "model_artefact['results'] = {}\n",
    "model_artefact['results']['auc_train'] = train_auc_score\n",
    "model_artefact['results']['auc_test'] = test_auc_score\n",
    "model_artefact['results']['auc_oot'] = oot_auc_score\n",
    "model_artefact['results']['gini_train'] = round(2*train_auc_score-1,3)\n",
    "model_artefact['results']['gini_test'] = round(2*test_auc_score-1,3)\n",
    "model_artefact['results']['gini_oot'] = round(2*oot_auc_score-1,3)\n",
    "model_artefact['hp_params'] = random_search.best_params_\n",
    "\n",
    "\n",
    "pprint.pprint(model_artefact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea194e4-669d-43a6-b31f-ef14f0d188aa",
   "metadata": {},
   "source": [
    "## save artefact to model bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff1665-3eef-4b13-8c78-3cfe19fd1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model_bank dir\n",
    "model_bank_directory = \"model_bank/\"\n",
    "\n",
    "if not os.path.exists(model_bank_directory):\n",
    "    os.makedirs(model_bank_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebf078-449b-4e6c-81fa-496d901e97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full path to the file\n",
    "file_path = os.path.join(model_bank_directory, model_artefact['model_version'] + '.pkl')\n",
    "\n",
    "# Write the model to a pickle file\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(model_artefact, file)\n",
    "\n",
    "print(f\"Model saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91f891-d285-46ef-8901-d19ec702176b",
   "metadata": {},
   "source": [
    "## test load pickle and make model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1da5b-0766-4ac5-b643-115daad8e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the pickle file\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_model_artefact = pickle.load(file)\n",
    "\n",
    "y_pred_proba = loaded_model_artefact['model'].predict_proba(X_oot_processed)[:, 1]\n",
    "oot_auc_score = roc_auc_score(y_oot, y_pred_proba)\n",
    "print(\"OOT AUC score: \", oot_auc_score)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df822718-3476-4b5d-8e1f-766d34982eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
