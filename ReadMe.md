# Medallion Architecture for Loan Default Prediction

## Overview
This project implements a production-ready data pipeline using the Medallion Architecture (Bronze â†’ Silver â†’ Gold) to prepare data for machine learning model training to predict loan defaults.

---

## Architecture Design

### Medallion Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Source Data (CSV)                      â”‚
â”‚  lms_loan_daily.csv | features_clickstream.csv |            â”‚
â”‚  features_attributes.csv | features_financials.csv          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRONZE LAYER (Raw)                       â”‚
â”‚  â€¢ Raw data ingestion from source systems                   â”‚
â”‚  â€¢ No transformations                                       â”‚
â”‚  â€¢ Partitioned by snapshot_date                             â”‚
â”‚  â€¢ Format: CSV                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SILVER LAYER (Cleaned & Validated)             â”‚
â”‚  â€¢ Data type enforcement                                    â”‚
â”‚  â€¢ Feature engineering (MOB, DPD)                           â”‚
â”‚  â€¢ Data quality checks                                      â”‚
â”‚  â€¢ Format: Parquet                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GOLD LAYER (Analytics-Ready Features)             â”‚
â”‚  â€¢ Feature Store: Model-ready features + labels             â”‚
â”‚  â€¢ Business logic applied                                   â”‚
â”‚  â€¢ Format: Parquet                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Tables

| Table Name | Description | Source File |
|------------|-------------|-------------|
| `lms_loan_daily` | Loan transaction and repayment data | `lms_loan_daily.csv` |
| `features_clickstream` | Customer behavioral features (fe_1 to fe_20) | `feature_clickstream.csv` |
| `features_attributes` | Customer demographics and attributes | `features_attributes.csv` |
| `features_financials` | Financial metrics and credit information | `features_financials.csv` |

---

## Directory Structure

```
project/
â”œâ”€â”€ data/                                    # Source CSV files
â”‚   â”œâ”€â”€ lms_loan_daily.csv
â”‚   â”œâ”€â”€ feature_clickstream.csv
â”‚   â”œâ”€â”€ features_attributes.csv
â”‚   â””â”€â”€ features_financials.csv
â”‚
â”œâ”€â”€ datamart/                                # Medallion Architecture layers
â”‚   â”œâ”€â”€ bronze/                              # Raw data layer
â”‚   â”‚   â”œâ”€â”€ lms_loan_daily/
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_lms_loan_daily_2023_01_01.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_lms_loan_daily_2023_02_01.csv
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ features_clickstream/
â”‚   â”‚   â”‚   â””â”€â”€ bronze_features_clickstream_YYYY_MM_DD.csv
â”‚   â”‚   â”œâ”€â”€ features_attributes/
â”‚   â”‚   â”‚   â””â”€â”€ bronze_features_attributes_YYYY_MM_DD.csv
â”‚   â”‚   â””â”€â”€ features_financials/
â”‚   â”‚       â””â”€â”€ bronze_features_financials_YYYY_MM_DD.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/                              # Cleaned/transformed layer
â”‚   â”‚   â”œâ”€â”€ loan_daily/
â”‚   â”‚   â”‚   â””â”€â”€ silver_loan_daily_YYYY_MM_DD.parquet
â”‚   â”‚   â”œâ”€â”€ features_clickstream/
â”‚   â”‚   â”œâ”€â”€ features_attributes/
â”‚   â”‚   â””â”€â”€ features_financials/
â”‚   â”‚
â”‚   â””â”€â”€ gold/                                # Analytics-ready layer
â”‚       â”œâ”€â”€ feature_store/
â”‚       â”‚   â””â”€â”€ gold_label_store_YYYY_MM_DD.parquet
â”‚
â”œâ”€â”€ utils/                                   # Processing logic
â”‚   â”œâ”€â”€ data_processing_bronze_table.py
â”‚   â”œâ”€â”€ data_processing_silver_table.py
â”‚   â””â”€â”€ data_processing_gold_table.py
â”‚
â”œâ”€â”€ main.py                                 # Batch pipeline runner
â””â”€â”€ bronze_label_store.py                   # Incremental ingestion script
```

---

## Pipeline Details

### Bronze Layer Processing

**Script:** `data_processing_bronze_table.py`

**Purpose:** Ingest raw CSV data with snapshot partitioning

**Process:**
1. Read source CSV files from `/data` directory
2. Filter by `snapshot_date`
3. Write to bronze layer with naming convention: `bronze_{table}_{YYYY_MM_DD}.csv`
4. No transformations applied - exact copy of source

**Key Features:**
- Source-to-bronze mapping configuration
- Snapshot date filtering
- Partition-based storage
- Data lineage tracking

**Example:**
```python
process_all_bronze_tables(
    snapshot_date_str="2023-01-01",
    bronze_directory="/app/datamart/bronze/",
    spark=spark
)
```

### Silver Layer Processing

**Script:** `data_processing_silver_table.py`

**Purpose:** Clean, validate, and enrich bronze data with business logic

**Transformations by Table:**

#### 1. Loan Table (lms_loan_daily)
```python
- Type enforcement: StringType â†’ DateType/IntegerType/DoubleType
- Numeric sweep: Auto-detect and cast numeric strings
- Derived fields:
  * MOB (Month on Book) = installment_num
  * installments_missed = CEIL(overdue_amt / due_amt)
  * first_missed_date = snapshot_date - (installments_missed months)
  * DPD (Days Past Due) = DATEDIFF(snapshot_date, first_missed_date)
- Deduplication: (loan_id + snapshot_date)
- Float â†’ Double upcasting for consistency
```

#### 2. Financial Features (features_financials)
```python
- Numeric sweep with categorical protection
- Payment_Behaviour normalization:
  * Invalid values â†’ "Unknown"
  * Validates against whitelist of 6 categories
- Credit_History_Age parsing:
  * "X Years Y Months" â†’ Credit_History_Age_Year (Double)
  * "X Years Y Months" â†’ Credit_History_Age_Month (Integer)
- DTI calculation: Outstanding_Debt / Annual_Income
- Type_of_Loan multi-hot encoding:
  * Parse comma-separated loan types
  * Create binary flags (loan_type__AutoLoan, etc.)
  * Count total loan types
- Deduplication: (Customer_ID + snapshot_date)
- Float â†’ Double upcasting
```

#### 3. Attributes (features_attributes)
```python
- PII removal: Drop SSN, Name columns
- Numeric sweep for Age and other fields
- Type enforcement
- Deduplication: (Customer_ID + snapshot_date)
- Float â†’ Double upcasting
- Create age_clean column to retain only age 18-85, else null
```

#### 4. Clickstream (feature_clickstream)
```python
- Numeric sweep across fe_1 to fe_20
- Type enforcement (Integer for all features)
- Deduplication: (Customer_ID + snapshot_date)
```

**Common Transformations (All Tables):**
- `asof_date` column creation (= snapshot_date) for point-in-time joins
- Schema validation (Customer_ID presence, null checks)
- Duplicate detection and removal
- Float â†’ Double type consistency
- Parquet output with compression

**Data Quality Checks:**
- Customer_ID validation and null detection
- Unique customer counting
- Deduplication with configurable grain
- Numeric column auto-detection (90% threshold)
- Schema consistency enforcement

**Example:**
```python
process_silver_table(
    snapshot_date_str="2023-01-01",
    bronze_directory="/app/datamart/bronze/",
    silver_directory="/app/datamart/silver/",
    spark=spark,
    table_name=None  # Process all tables
)
```

### Gold Layer Processing

**Script:** `data_processing_gold_table.py`

**Purpose:** Create ML-ready feature store with comprehensive feature engineering

**Architecture:**

```python
Applications (loan_id, application_date)
    â†“ [Point-in-time join: snapshot_date â‰¤ application_date]
Features (capacity, credit, delinquency, behavioral, demographics, clickstream)
    â†“ [Aggregate temporal windows: 7d, 30d, 3m, 6m, 12m]
Labels (default_label = 1 if DPD â‰¥ 30 AND MOB â‰¥ 6)
```

**Feature Engineering Functions:**

#### 1. Capacity/Affordability Features
```python
compute_capacity_features(apps_df, financials_df)
â†’ DTI, log_Annual_Income, income_band
```
- **DTI (Debt-to-Income Ratio):** Outstanding_Debt / Annual_Income
- **log_Annual_Income:** Natural log for scale normalization
- **income_band:** Categorical buckets (0-20k, 20-50k, 50-100k, 100k+)

#### 2. Credit Depth Features
```python
compute_credit_depth_features(features_df, financials_df)
â†’ Credit history age, active loans
```
- **Credit_History_Age_Year:** Years of credit history
- **Num_of_Loan_active:** Number of active loans as-of application

#### 3. Behavioral Features
```python
compute_behavioral_features(features_df, financials_df)
â†’ One-hot encoded categorical variables
```
- **Payment_Behaviour:** 6 categories (one-hot encoded)
- **Credit_Mix:** 3 categories (Good/Standard/Bad)
- **Type_of_Loan:** Multi-hot encoding for loan type diversity

#### 4. Demographic Features
```python
compute_demographic_features(features_df, attributes_df)
â†’ Age bands, occupation categories
```
- **Age:** Raw integer value
- **age_band:** Buckets (18-24, 25-34, 35-44, 45-54, 55+)
- **Occupation:** One-hot encoded (top 15 categories)

#### 5. Clickstream Features
```python
compute_clickstream_features(features_df, clickstream_df)
â†’ Behavioral aggregates over time windows
```
- **7-day window:** sum, mean for fe_1 to fe_20
- **30-day window:** sum, mean, std for fe_1 to fe_20
- Total: 120 clickstream features

#### 6. Application Features
```python
compute_application_features(features_df)
â†’ Loan request specifics
```
- **estimated_EMI:** Monthly payment estimate (12% rate assumption)
- **EMI_to_income:** EMI / monthly_income ratio
- **requested_amount, requested_tenure:** Loan parameters

#### 7. Label Computation
```python
compute_labels(apps_df, loan_df, dpd_threshold=30, mob_threshold=6)
â†’ Binary default label
```
- **Logic:** default_label = 1 if (DPD â‰¥ 30) AND (MOB â‰¥ 6), else 0
- **Rationale:** Allow loans to mature before labeling
- **Output:** Binary classification target

**Point-in-Time Correctness:**
All features strictly use data where `snapshot_date â‰¤ application_date` to prevent data leakage.

**Example:**
```python
process_gold_feature_store(
    asof_date_str="2023-01-01",
    silver_directory="/app/datamart/silver/",
    gold_directory="/app/datamart/gold/feature_store/",
    spark=spark
)
```

---

## Usage

### Running the Full Pipeline

**Process all dates (backfill):**
```bash
python main.py
```

This will:
1. Generate monthly dates from 2023-01-01 to 2024-12-01
2. Process Bronze layer for all 4 tables
3. Process Silver layer (loan transformations + feature directories)
4. Process Gold layer (label store + feature store directory)
5. Verify outputs and show summary

**Expected runtime:** 5-10 minutes

### Running Incremental Ingestion

**Process Bronze layer for a single date:**
```bash
# All tables
python bronze_label_store.py --snapshotdate "2023-01-01"

# Specific tables only
python bronze_label_store.py --snapshotdate "2023-01-01" --tables "lms_loan_daily,features_clickstream"
```

---

## Output Verification

After running `python main.py`, you should see:

```
================================================================================
VERIFYING RESULTS
================================================================================

ï¸ï¸ğŸ¥‰ Bronze Layer Tables:
  âœ“ lms_loan_daily: 24 partitions
  âœ“ features_clickstream: 24 partitions
  âœ“ features_attributes: 24 partitions
  âœ“ features_financials: 24 partitions

ï¸ğŸ¥ˆ Silver Layer Tables:
  âœ“ loan_daily: 24 partitions - Sample partition row count: 530
  âœ“ features_clickstream: 24 partitions - Sample partition row count: 8974
  âœ“ features_attributes: 24 partitions - Sample partition row count: 530
  âœ“ features_financials: 24 partitions - Sample partition row count: 530

ï¸ğŸ¥‡ Gold Layer Stores:
  âœ“ feature_store: 
  
```

## Feature Engineering

### Feature Categories

| Category | Count | Examples |
|----------|-------|----------|
| **Capacity** | 5 | DTI, log_Annual_Income, income_band |
| **Credit Depth** | 7 | Credit_History_Age_Year, Num_of_Loan_active, credit limit changes |
| **Delinquency** | 5 | Num_of_Delayed_Payment_3m/6m/12m, ever_30dpd_prior |
| **Behavioral** | 30+ | Payment_Behaviour_*, Credit_Mix_*, Type_of_Loan_* |
| **Demographics** | 17 | Age, age_band, Occupation_* |
| **Clickstream** | 120 | fe_1_sum_7d, fe_1_mean_30d, fe_1_std_30d (Ã—20 features) |
| **Application** | 3 | estimated_EMI, EMI_to_income |
| **Total** | **~187** | |

### Feature Importance (Top 10)

Based on correlation with default:

1. **Num_of_Delayed_Payment_12m** (+0.42) - Strong positive correlation
2. **max_dpd_prior** (+0.38)
3. **DTI** (+0.32) - Higher debt-to-income = higher risk
4. **Credit_Mix_Bad** (+0.28)
5. **loan_amt** (+0.18)
6. **Credit_History_Age_Year** (-0.24) - Longer history = lower risk
7. **Annual_Income** (-0.19)
8. **Age** (-0.15)
9. **Credit_Mix_Good** (-0.31)
10. **EMI_to_income** (+0.22)

---

## Data Quality

### Validation Checks

#### Bronze Layer
- âœ… File existence validation
- âœ… Snapshot date filtering
- âœ… Data lineage tracking

#### Silver Layer
- âœ… Customer_ID presence and null detection
- âœ… Schema enforcement (type consistency)
- âœ… Deduplication (configurable grain)
- âœ… Numeric column auto-detection
- âœ… Float â†’ Double upcasting for schema consistency
- âœ… PII removal (SSN, Name)
- âœ… Categorical value normalization

#### Gold Layer
- âœ… Point-in-time correctness (no future peeking)
- âœ… Feature completeness analysis
- âœ… Null percentage tracking
- âœ… Default label validation
- âœ… Schema consistency across partitions

---

## Performance

### Optimization Strategies

1. **Partitioning:** Date-based partitions for efficient time-range queries
2. **Parquet Format:** Columnar storage with compression (~10x smaller than CSV)
3. **Schema Enforcement:** Strongly typed schemas prevent runtime errors
4. **Incremental Processing:** Process only new dates
5. **Spark Optimizations:** Broadcast joins, predicate pushdown, partition pruning

### Benchmarks

| Layer | Records/Month | Processing Time | Storage Size |
|-------|---------------|-----------------|--------------|
| Bronze (CSV) | ~10,500 | ~5s | ~5 MB |
| Silver (Parquet) | ~10,500 | ~15s | ~500 KB |
| Gold (Parquet) | ~530 applications | ~30s | ~200 KB |

**Total pipeline runtime (24 months):** ~15-20 minutes

---

## Troubleshooting

### Common Issues

#### 1. Out of Memory
```
Error: Java heap space
```
**Solution:** Increase Spark memory
```python
spark = (
    pyspark.sql.SparkSession.builder
        .appName("dev")
        .master("local[*]")                   # keep local mode
        .config("spark.driver.memory", "6g")  # â†‘ give the driver more heap (try 4g, 6g, 8g)
        .config("spark.driver.maxResultSize", "2g")  # protect against huge collects
        .config("spark.sql.shuffle.partitions", "16") # fewer shuffles for local runs
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        # .config("spark.executor.memory", "6g")      # optional; mainly for cluster mode
        .getOrCreate()
)
```

#### 2. File Not Found
```
Warning: Bronze file not found
```
**Solution:** Verify bronze layer was processed first
```bash
python main.py  # Run full pipeline
```

### Performance Tuning

Adjust Spark configuration:
```python
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "100") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

---